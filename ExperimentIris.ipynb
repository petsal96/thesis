{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":77434,"status":"ok","timestamp":1683640950039,"user":{"displayName":"Petrus Salminen","userId":"07126567515321158789"},"user_tz":-120},"id":"f6eH7BYxYStC","outputId":"4d49473e-6c85-4670-c76a-247098f86c28"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["import tensorflow as tf\n","from tensorflow.keras import Input, Model\n","from tensorflow.keras.constraints import MaxNorm\n","from tensorflow.keras.layers import Flatten, Dense \n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.activations import relu, tanh\n","from tensorflow.keras.initializers import Constant, RandomNormal\n","from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n","from tensorflow.keras.optimizers import SGD\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import log_loss\n","from multiprocessing import Process, Value\n","from datetime import datetime\n","import numpy as np\n","import math\n","import os\n","from PIL import Image\n","from google.colab import drive\n","import pickle\n","import pandas as pd\n","\n","drive.mount(\"/content/drive\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PBaKUkMCYjj7"},"outputs":[],"source":["def normalized_relu(x):\n","  return math.sqrt(2) * relu(x)\n","\n","def normalized_tanh(x):\n","  return tanh(x) / 0.6279\n","\n","def build_model(input_size, r = 32):\n","  # define model:\n","  model = Sequential()\n","  # add hidden fully connected layer:\n","  model.add(Dense(r, activation = normalized_relu, kernel_initializer = RandomNormal(0, math.sqrt(input_size[-1] / np.prod(input_size))), bias_initializer = Constant(0), bias_constraint = MaxNorm(0), input_shape = input_size))\n","  # add output layer:\n","  model.add(Dense(3))\n","  return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ATw39Ad_YnP-"},"outputs":[],"source":["# data:\n","!cp /content/drive/MyDrive/iris/iris.csv /content"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sh7J1DfQYnUu"},"outputs":[],"source":["# input transformation:\n","def load_data(path):\n","  df = pd.read_csv(path)\n","  df[df[\"Species\"] == \"Iris-setosa\"] = 0\n","  df[df[\"Species\"] == \"Iris-versicolor\"] = 1\n","  df[df[\"Species\"] == \"Iris-virginica\"] = 2\n","  data = df.to_numpy(dtype = np.float32)\n","  features = data[:, 1:-1][:, :, None]\n","  labels = data[:, -1].astype(int)\n","  return features, labels\n","\n","def sphere_transformation(data):\n","    trans_data = np.empty(data.shape[0:-1] + (data.shape[-1] + 1,))\n","    trans_data[..., 0] = np.cos(data[..., 0])\n","    for i in range(1, data.shape[-1]):\n","        trans_data[..., i] = np.prod(np.sin(data[..., :i]), axis = -1) * np.cos(data[..., i])\n","    trans_data[..., -1] = np.prod(np.sin(data), axis = -1)\n","    return trans_data\n","\n","features, labels = load_data(\"iris.csv\")\n","features = (2 * features / np.max(features, axis = 0) - 1) * math.pi\n","\n","train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.2)\n","\n","train_features = sphere_transformation(train_features)\n","train_features = train_features.reshape((train_features.shape[0], -1))\n","test_features = sphere_transformation(test_features)\n","test_features = test_features.reshape((test_features.shape[0], -1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zDu_bercfg3K"},"outputs":[],"source":["# setups:\n","input_size = train_features[0].shape\n","rs = (2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048)\n","n_folds = 5\n","epochs = 500\n","\n","# hyperparams:\n","batch_sizes = (16, 32, 64)\n","decay_epochs = (25, 50, 75, 100)\n","Cs = (1E-3, 1E-2, 1E-1, 1, 1E+1, 1E+2, 1E+3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zBTpYNnhO49p"},"outputs":[],"source":["# read current results:\n","if os.path.exists(\"/content/drive/MyDrive/Irisrelu.pickle\"):\n","  with open(\"/content/drive/MyDrive/Irisrelu.pickle\", \"rb\") as file:\n","    results = pickle.load(file)\n","else:\n","  results = {}\n","  folds = StratifiedKFold(n_splits = n_folds, shuffle = True)\n","  splits = [(train, val) for (train, val) in folds.split(train_features, train_labels)]\n","  results[\"HYP_LAST_LOSSES\"] = []\n","  results[\"HYP_LAST_SPLITS\"] = splits\n","  results[\"HYP_FULL_LOSSES\"] = []\n","  results[\"HYP_FULL_SPLITS\"] = splits\n","  results[\"LAST_LOSSES\"] = []\n","  results[\"FULL_LOSSES\"] = []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XVg6HqCrfeRO"},"outputs":[],"source":["# learning rate scheduler:\n","max_lr = 1E-2\n","min_lr = 1E-5\n","def lr_decay(epoch, lr, epochs, max_lr, min_lr):\n","  if epoch == 0:\n","    return lr\n","  decay = (min_lr / max_lr) ** (1 / (epochs - 1))\n","  if lr * decay \u003e min_lr:\n","    lr *= decay\n","  return lr\n","\n","# full model evaluation:\n","def full_model_eval(r, input_size, train_features, train_labels, test_features, test_labels, de, bs, test_loss, test_accuracy): \n","  # split training set:\n","  X_train, X_val, y_train, y_val = train_test_split(train_features, train_labels, test_size = 0.25)\n","  # train model:\n","  full_model = build_model(input_size, r)\n","  full_model.compile(optimizer = SGD(learning_rate = max_lr), loss = SparseCategoricalCrossentropy(from_logits = True), metrics = [\"accuracy\"])\n","  full_model.fit(X_train, y_train, epochs = epochs, batch_size = bs, validation_data = (X_val, y_val), callbacks = [EarlyStopping(monitor = \"val_loss\", min_delta = 0.001, patience = 10, restore_best_weights = True), LearningRateScheduler(lambda epoch, lr: lr_decay(epoch, lr, de, max_lr, min_lr))], verbose = 0)\n","  # evaluate on test set:\n","  test_loss.value, test_accuracy.value = full_model.evaluate(test_features, test_labels, verbose = 0)\n","\n","# last layer model evaluation:\n","def last_layer_eval(r, input_size, train_features, train_labels, test_features, test_labels, C, test_loss, test_accuracy): \n","  # train model:\n","  last_layer_model = build_model(input_size, r)\n","  last_layer_model = Model(inputs = last_layer_model.input, outputs = last_layer_model.layers[-2].output)\n","  X_train = last_layer_model.predict(train_features, verbose = 0)\n","  y_train = np.squeeze(train_labels)\n","  pipe = make_pipeline(StandardScaler(), LogisticRegression(C = C, fit_intercept = False, max_iter = 10000, verbose = 0))\n","  pipe.fit(X_train, y_train)\n","  # evaluate on test set:\n","  X_test = last_layer_model.predict(test_features, verbose = 0)\n","  y_test = np.squeeze(test_labels)\n","  probs_test = pipe.predict_proba(X_test)\n","  test_loss.value = log_loss(y_test, probs_test)\n","  test_accuracy.value = pipe.score(X_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"FeyTFuiqapIc"},"outputs":[],"source":["# optimize last layer hyperparams:\n","for i, (train, val) in enumerate(results[\"HYP_LAST_SPLITS\"]):\n","\n","  if len(results[\"HYP_LAST_LOSSES\"]) \u003c= i:\n","    results[\"HYP_LAST_LOSSES\"].append([])\n","\n","  for j, r in enumerate(rs):\n","\n","    if len(results[\"HYP_LAST_LOSSES\"][i]) \u003c= j:\n","      results[\"HYP_LAST_LOSSES\"][i].append([])\n","\n","    # train last layer models:\n","    for k, C in enumerate(Cs):\n","\n","      if len(results[\"HYP_LAST_LOSSES\"][i][j]) \u003e k:\n","        continue\n","\n","      before = datetime.now()\n","      loss = Value(\"d\", math.inf)\n","      accuracy = Value(\"d\", 0.)\n","      p = Process(target = last_layer_eval, args = (r, input_size, train_features[train], train_labels[train], train_features[val], train_labels[val], C, loss, accuracy))\n","      p.start()\n","      p.join()\n","      results[\"HYP_LAST_LOSSES\"][i][j].append(loss.value)\n","      after = datetime.now()\n","\n","      with open(\"/content/drive/MyDrive/Irisrelu.pickle\", \"wb\") as file:\n","        pickle.dump(results, file, protocol = pickle.HIGHEST_PROTOCOL)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ZrwkUl2q1Alj"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1000. 1000. 1000. 1000. 1000. 1000. 1000. 1000.    1. 1000. 1000.]\n"]}],"source":["C_best = np.array(Cs)[np.argmin(np.mean(results[\"HYP_LAST_LOSSES\"], axis = 0), axis = -1)]\n","print(C_best)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"3boygL_3CrTb"},"outputs":[],"source":["# optimize full model hyperparams:\n","for i, (train, val) in enumerate(results[\"HYP_FULL_SPLITS\"]):\n","\n","  if len(results[\"HYP_FULL_LOSSES\"]) \u003c= i:\n","    results[\"HYP_FULL_LOSSES\"].append([])\n","\n","  for j, r in enumerate(rs):\n","\n","    if len(results[\"HYP_FULL_LOSSES\"][i]) \u003c= j:\n","      results[\"HYP_FULL_LOSSES\"][i].append([])\n","\n","    for k, bs in enumerate(batch_sizes):\n","\n","      if len(results[\"HYP_FULL_LOSSES\"][i][j]) \u003c= k:\n","        results[\"HYP_FULL_LOSSES\"][i][j].append([])\n","      \n","      # train full models:\n","      for l, de in enumerate(decay_epochs):\n","\n","        if len(results[\"HYP_FULL_LOSSES\"][i][j][k]) \u003e l:\n","          continue\n","        \n","        before = datetime.now()\n","        loss = Value(\"d\", math.inf)\n","        accuracy = Value(\"d\", 0.)\n","        p = Process(target = full_model_eval, args = (r, input_size, train_features[train], train_labels[train], train_features[val], train_labels[val], de, bs, loss, accuracy))\n","        p.start()\n","        p.join()\n","        results[\"HYP_FULL_LOSSES\"][i][j][k].append(loss.value)\n","        after = datetime.now()\n","\n","        with open(\"/content/drive/MyDrive/Irisrelu.pickle\", \"wb\") as file:\n","          pickle.dump(results, file, protocol = pickle.HIGHEST_PROTOCOL)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"lKmj6gekDaYq"},"outputs":[{"name":"stdout","output_type":"stream","text":["[32 32 16 16 16 16 64 16 16 32 16]\n","[100  50  50 100 100  75 100 100  50  75  25]\n"]}],"source":["means = np.mean(results[\"HYP_FULL_LOSSES\"], axis = 0)\n","indices = np.argmin(means.reshape((means.shape[0], -1)), axis = -1)\n","indices = np.unravel_index(indices, means.shape[1:])\n","batch_size_best = np.array(batch_sizes)[indices[0]]\n","decay_epochs_best = np.array(decay_epochs)[indices[1]]\n","print(batch_size_best)\n","print(decay_epochs_best)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sx0QYA7f1Fhl"},"outputs":[],"source":["# optimize hyperparameters:\n","repetitions = 25\n","\n","for i, rep in enumerate(range(repetitions)):\n","\n","  if len(results[\"LAST_LOSSES\"]) \u003c= i:\n","    results[\"LAST_LOSSES\"].append([])\n","  if len(results[\"FULL_LOSSES\"]) \u003c= i:\n","    results[\"FULL_LOSSES\"].append([])\n","\n","  for j, r in enumerate(rs):\n","\n","    if len(results[\"LAST_LOSSES\"][i]) \u003c= j:\n","      loss = Value(\"d\", math.inf)\n","      accuracy = Value(\"d\", 0.)\n","      p = Process(target = last_layer_eval, args = (r, input_size, train_features, train_labels, test_features, test_labels, C_best[j], loss, accuracy))\n","      p.start()\n","      p.join()\n","      results[\"LAST_LOSSES\"][i].append(loss.value)\n","\n","    if len(results[\"FULL_LOSSES\"][i]) \u003c= j:\n","      loss = Value(\"d\", math.inf)\n","      accuracy = Value(\"d\", 0.)\n","      p = Process(target = full_model_eval, args = (r, input_size, train_features, train_labels, test_features, test_labels, decay_epochs_best[j], batch_size_best[j], loss, accuracy))\n","      p.start()\n","      p.join()\n","      results[\"FULL_LOSSES\"][i].append(loss.value)\n","\n","print(np.mean(results[\"LAST_LOSSES\"], axis = 0))\n","print(np.mean(results[\"FULL_LOSSES\"], axis = 0))\n","\n","with open(\"/content/drive/MyDrive/Irisrelu.pickle\", \"wb\") as file:\n","  pickle.dump(results, file, protocol = pickle.HIGHEST_PROTOCOL)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LCTDgddkm-io"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","x = rs\n","y1 = np.mean(results[\"LAST_LOSSES\"], axis = 0)\n","y2 = np.mean(results[\"FULL_LOSSES\"], axis = 0)\n","plt.plot(x, y1)\n","plt.plot(x, y2)\n","plt.savefig(\"scatterIrisrelu.png\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B5Df6nc1pnhi"},"outputs":[],"source":["!cp /content/scatterIrisrelu.png /content/drive/MyDrive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F_nf529zctdO"},"outputs":[],"source":["from google.colab import runtime\n","runtime.unassign()"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","name":"","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}